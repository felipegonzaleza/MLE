{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"week1_V1.ipynb","provenance":[{"file_id":"1A5aKkAXYKm5ju0TNG-1yU44-hnWEhh__","timestamp":1603344980309}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lNnTTMT9dnM4"},"source":["# Today you are a Data Scientist at Tesla! \n","## You have been assigned a new project to look at car sales from Quarters 1-2 in California for 2019 to make predictions as to which cars will be sold more than the others in Q3 and Q4, to ensure enough inventory to meet demands!"]},{"cell_type":"markdown","metadata":{"id":"AQqh5DMaq9QW"},"source":["### If running this notebook in Google Colab, run the following cell first to mount your Google Drive"]},{"cell_type":"code","metadata":{"id":"Mqah54vN8CVB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627687605649,"user_tz":420,"elapsed":197,"user":{"displayName":"Spencer Kent","photoUrl":"","userId":"07841346171340846448"}},"outputId":"8633c489-6439-4a66-dafd-cba26bc7e88d"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NliK7yTn8LMn"},"source":["^^ This mounts your Google Drive at the location */content/drive* on the virtual machine running this notebook."]},{"cell_type":"markdown","metadata":{"id":"v-eh9IKUfTdQ"},"source":["# Task 1: Load data and wrangle training and test sets"]},{"cell_type":"markdown","metadata":{"id":"OI6NP0JBdnM5"},"source":["### Import some modules we'll make use of"]},{"cell_type":"code","metadata":{"id":"WdNx3CHEdnM5"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dM4fGT-ldnM8"},"source":["### Read in the CSV file containing the California sales data for Quarters 1 and 2\n","\n","Then examine the data's shape and first few rows"]},{"cell_type":"code","metadata":{"id":"-s13HIEBdtGd"},"source":["# my copy of the Tesla sales data is located in my drive at /Datasets/week_1/\n","df_sales = pd.read_csv('/content/drive/My Drive/Datasets/week_1/sales_Q12_2019.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HdP5mSNQePPU"},"source":["print(df_sales.shape)\n","df_sales.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9zWKdbKgHQCj"},"source":["print(df_sales.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dF7w0UQndnM_"},"source":["### Begin cleaning the data\n","\n","Eliminate the `'dealer_state'` and `'date'` columns. The former is useless to our model, since we already know that our dataset is restricted to California sales. While we could possibly extract useful information from the `'date'` column (for example, to determine whether more cars are sold on weekends than weekdays), we'll be focusing on car configurations in this exercise."]},{"cell_type":"code","metadata":{"id":"i-DThsrHdnNA"},"source":["df_sales = df_sales.drop(columns=['dealer_state','date'])\n","print(df_sales.shape)\n","df_sales.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qqdkmnCodnNC"},"source":["### Read in the CSV file containing the California sales data for Quarters 3 and 4\n","\n","The `'dealer_state'` and `'date'` columns have already been eliminated in this dataset, so you don't need to worry about them here. Examine the data's shape and first few rows."]},{"cell_type":"code","metadata":{"id":"bJpTgHxAdnND"},"source":["# my copy of the Tesla sales data is located in my drive at /Datasets/week_1/\n","df_pred = pd.read_csv('/content/drive/My Drive/Datasets/week_1/sales_Q34_2019.csv')\n","print(np.shape(df_pred))\n","print(df_pred.columns)\n","df_pred.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kcmsU-T1dnNF"},"source":["### Set up a regression problem that uses car IDs to predict proportion of # sold. \n","\n","You've probably noticed that 73 of the 74 columns in our Q12 and Q34 datasets\n","are one-hot-encoded representations of the car's `'main_type'`, `'engine'`, and \n","`'sales_version'` values. This could be viewed as a unique ID for each type of car. You've probably also noticed that the final column is \n","the car's `'MSRP'`, or manufacturer's (Tesla's, in this case) suggested retail price. You could do several things with this data--we're asking you to fit a model of the **proportion of cars sold** based on the unique ID. You'll fit the model on Q12 data and test it on Q34 data. We won't use MSRP.\n","\n","Each row in the dataset represents the sale of a single car. So, we can count up\n","the number of rows with a particular ID, and this gives us the number of times that specific type of car was sold in California during that time period. This number divided by the total number sold is the proportion of sales attributed to this particular type of car.\n","\n","<!-- \n","\n","\n","Consolidate data by finding numbers of unique car combinations sold for training and test data sets\n","\n","\n","\n","However, each row represents the sale of a single car. If we define a unique car type by its combination of `'main_type'`, `'engine'`, and `'sales_version'` values, the number of rows displaying that combination corresponds to the number of times during that half of the year that that distinct type of car was sold in California. Therefore, we can make training and test datasets where each row now corresponds to a unique car type, and the target value is how many times that car type was sold in California during a given half of the year.\n","\n","## This task requires data wrangling!\n","## Create functions that read the Q12 (df_sales) and Q34 (df_pred) data sets and create train_X, train_Y, test_X and test_Y, respectively. Use pandas and NumPy as needed. -->\n","\n","**Create a Python function called ``get_features_and_targets`` that takes in a quarterly sales dataframe and produces matrices $\\mathbf{X}$ and $\\mathbf{Y}$, where $\\mathbf{X}$ has in each row a unique car ID and the corresponding row of $\\mathbf{Y}$ has that specific car's proportion of total sales for the quarter**"]},{"cell_type":"code","metadata":{"id":"Mo5c3VhBkB33"},"source":["def get_features_and_targets(df):\n","  # YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"toTX3W89fX2P"},"source":["### Use your function to create $\\mathbf{X}$, $\\mathbf{Y}$, pairs for both the training data and the test data:"]},{"cell_type":"code","metadata":{"id":"Rf6Vj0ujhJ77"},"source":["train_X, train_Y = get_features_and_targets(df_sales)\n","test_X, test_Y = get_features_and_targets(df_pred)\n","print(np.shape(train_X))\n","print(f\"Number of unique cars in Q12 = {len(train_Y)}\")\n","print(f\"Number of unique cars in Q34 = {len(test_Y)}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8d7BSjsQdnNV"},"source":["Okay, so let's note that the Q12 and Q34 datasets contain differing numbers of distinct cars. Clearly, some new models were introduced by Q3, but were any discontinued by the end of Q2? Let's find out."]},{"cell_type":"markdown","metadata":{"id":"y7Z-Trs9o821"},"source":["Print the number of cars that were sold in either Q12 or Q34, or both. In mathematical notation, if the set of cars sold in Q12 is $\\mathcal{A}$ and the set of cars sold in Q34 is $\\mathcal{B}$, we're asking for the size of the *union* of these two sets $|\\mathcal{A} \\cup \\mathcal{B}|$. The notation $|\\cdot|$ indicates measurement of the size of a set, the number of distinct items. (Mathematicians usually refer to it as the *cardinality*)"]},{"cell_type":"code","metadata":{"id":"KOEhFSxin_te"},"source":["## YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8KgAprxHqcv2"},"source":["A handy little fact from set theory is that $|\\mathcal{A} \\cup \\mathcal{B}| = |\\mathcal{A}| + |\\mathcal{B}| - |\\mathcal{A} \\cap \\mathcal{B}|$, where $\\cap$ is the *intersection* of $\\mathcal{A}$ and $\\mathcal{B}$, things that are in **both** $\\mathcal{A}$ and $\\mathcal{B}$. Use this fact, and what you've computed above, to print the number of models that were sold in both Q12 and Q34."]},{"cell_type":"code","metadata":{"id":"56A3357NswMG"},"source":["## YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"clPi1NVCunoy"},"source":["How many cars that were sold in Q12 were discontinued by Q34?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VmMdZNuzu5Z-","executionInfo":{"status":"ok","timestamp":1627687607185,"user_tz":420,"elapsed":12,"user":{"displayName":"Spencer Kent","photoUrl":"","userId":"07841346171340846448"}},"outputId":"cf6cf2d7-1b6c-472a-c1d7-dfd5fa885816"},"source":["## YOUR CODE HERE"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The number of cars discontinued in Q34 was 9\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"04zz5QOkvtij"},"source":["How many cars were launched in Q34?"]},{"cell_type":"code","metadata":{"id":"stRzG8THvyeA"},"source":["## YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3qn6SNBNdnNb"},"source":["# Task 2: Visualize the training and test targets any way you see fit"]},{"cell_type":"markdown","metadata":{"id":"OzsF0PfoyoyS"},"source":["This is super open-ended and we're not expecting a particular visualization, just show us what comes comes to your mind!"]},{"cell_type":"code","metadata":{"id":"oFVGTPYV2jrF"},"source":["## YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9g-2fwgydnNh"},"source":["# Task 3: Fit a linear model with gradient descent"]},{"cell_type":"markdown","metadata":{"id":"oriL0PcVOlw-"},"source":["Set hyperparameters for learning rate and maximum number of iterations through the training data."]},{"cell_type":"code","metadata":{"id":"QKIv3936OfVk"},"source":["# these are good starting values, you can play around with them though\n","s_learning_rate = 0.001\n","s_max_iteration = 1000"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lx-wS125dnNk"},"source":["### Hypothesis Function\n","\n","Define your hypothesis function $h(\\cdot)$ (which you use to make predictions $\\hat{\\mathbf{Y}}$ as the matrix product of your feature data $\\mathbf{X}$ and parameters $\\boldsymbol{\\theta}$. $\\boldsymbol{\\theta}$, which you'll initialize in the training loop (below) is a column vector, one for every feature in the training data, plus one for bias."]},{"cell_type":"code","metadata":{"id":"SI8oTUQsdnNk"},"source":["# Define your hypothesis function according to the instructions above\n","def h(theta, X):\n","  ## YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7LW5F5uhdnNm"},"source":["Define your loss function as **half** the MSE (mean squared error) between your actual and predicted $\\mathbf{Y}$ values. \n","\n","Recall that the predicted $\\mathbf{Y}$ values, $\\hat{\\mathbf{Y}}$, are a function of $\\boldsymbol{\\theta}$ and $\\mathbf{X}$.\n"]},{"cell_type":"code","metadata":{"id":"KzRpZec4dnNm"},"source":["# Loss Function\n","def loss (theta, X, Y) :\n","  ## YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PCLgRXivdnNp"},"source":["### Gradient of Hypothesis Function\n","\n","One can verify through straightforward (if somewhat tedious) multivariable calculus that the gradient of the loss function $J$ with respect to the parameters $\\theta$ is \n","\n","$$ \\frac{\\partial J}{\\partial \\boldsymbol{\\theta}} = - \\frac{1}{m} X^T \\cdot (Y - \\hat{Y})$$\n","\n","Where $m$ is the number of data samples, the number of rows in $\\mathbf{X}$ and $\\mathbf{Y}$.\n","\n","Note that the $\\mathbf{X}$ here is the one that has been augmented with a bias column. \n","\n","Set up a function ``gradient`` to compute this gradient."]},{"cell_type":"code","metadata":{"id":"3zYnvBa1dnNp"},"source":["def gradient (theta, X, Y) :\n","  ## YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fo-NxmSLdnNr"},"source":["### Gradient Descent\n","\n","Complete the function ``stochastic_gradient_descent`` below, to train your linear regression model with gradient descent, i.e. calculate $\\frac{\\partial J}{\\partial \\theta}$ and update $\\theta$. Recall that the general gradient descent update formula is $\\theta := \\theta - \\alpha \\frac{\\partial J}{\\partial \\theta}$, with $\\alpha$ the stepsize. We've provided the skeleton of a stochastic gradient descent function, but you're welcome to experiment with batch and/or minibatch gradient descent. Also recall that the aforementioned gradient descent methods differ in how frequently they calculate $\\frac{\\partial J}{\\partial \\theta}$ and update $\\theta$. Notice in the first step we initialize $\\boldsymbol{\\theta}$ to all zeros and we temporarily prepend a column of $1$'s to the features, which corresponds to the bias parameter."]},{"cell_type":"code","metadata":{"id":"g6hbmDgkdnNs"},"source":["def stochastic_gradient_descent(X, Y, learning_rate, max_iteration, print_interval):\n","  theta = np.zeros((X.shape[1]+1, 1))\n","  X_with_ones = np.hstack([np.ones([X.shape[0], 1]), X])  # prepend a column of 1s. This is just to make the math more compact, your original data is still X[:, 1:]\n","  # Initialize the cost as an array of zeros, one for each iteration through the dataset\n","  cost = np.zeros(max_iteration)\n","  # Loop over the dataset\n","  for i in range(max_iteration):\n","    # Loop over each row in the dataset\n","    for j in range(X.shape[0]):\n","      # Compute the gradient from the current row in X and the associated Y value\n","      # Make sure that both X and Y are represented as 2D row vectors\n","      ## YOUR CODE HERE\n","      # Update theta\n","      ## YOUR CODE HERE\n","    # Update the cost array for the current iteration\n","    cost[i] = loss(theta, X_with_ones, Y)\n","    if i % print_interval == 0 :\n","      print('iteration : ', i, ' loss : ', loss(theta, X_with_ones, Y)) \n","  return theta, cost"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O084UJ8UdnNt"},"source":["s_theta, s_cost = stochastic_gradient_descent(train_X, train_Y, s_learning_rate, s_max_iteration, 100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aoRR-pP8P2C2"},"source":["plt.stem(np.squeeze(s_theta))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B6D2kqejdnNv"},"source":["### Generate Predictions from Test Data"]},{"cell_type":"code","metadata":{"id":"m-u3iTbNdnNw"},"source":["# remember that s_theta should be applied to features that have a prepended column of 1's\n","pred_Y_from_GD = h(s_theta, np.hstack([np.ones([test_X.shape[0], 1]), test_X]))\n","# Set any negative predictions to 0\n","pred_Y_from_GD[pred_Y_from_GD<0] = 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ImYOho6dnNy"},"source":["### Visualize the predicted and actual test labels"]},{"cell_type":"code","metadata":{"id":"o3t83fxcr6dd"},"source":["from sklearn.metrics import mean_squared_error as MSE\n","from sklearn.metrics import r2_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ljk24gJ4dnNy"},"source":["# compare predictions pred_Y_from_GD to test_Y. Report MSE and R^2 score\n","## YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QNFkAA7gdnN0"},"source":["# Task 4: Normal Equations\n","\n","Since our training dataset isn't very large, let's generate predictions using the normal equations: \n","\n","$$\\boldsymbol{\\theta} = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot Y$$ \n","$$\\hat{Y} = X \\cdot \\boldsymbol{\\theta}$$\n","\n","and see how they compare to the predictions which we obtained from gradient descent. Remember we still have a bias term, so $\\boldsymbol{\\theta}$ is of size 74x1 (73 for the unique ID features, 1 for the bias)"]},{"cell_type":"code","metadata":{"id":"ez7XC2qQdnN1"},"source":["def normal_equations_solution(X, Y):\n","  X_with_ones = np.hstack([np.ones([train_X.shape[0], 1]), train_X])\n","  ## YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cAhtVznMZGtB"},"source":["# Compute the predicted Y values\n","n_theta = normal_equations_solution(train_X, train_Y)\n","pred_Y_from_N = np.matmul(np.hstack([np.ones([test_X.shape[0], 1]), test_X]), n_theta)\n","\n","# Set any negative predictions to 0\n","pred_Y_from_N[np.where(pred_Y_from_N<0)]=0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lu9NjgsJdnN3","scrolled":true},"source":["# Plot predictions compared to test_Y, and also print MSE and R^2 score\n","## YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i3ofJxdUdnN6"},"source":["### Regularized Normal Equations"]},{"cell_type":"code","metadata":{"id":"VyIX4LaAdnN6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627687637973,"user_tz":420,"elapsed":9,"user":{"displayName":"Spencer Kent","photoUrl":"","userId":"07841346171340846448"}},"outputId":"4011e415-a117-43ae-d5ff-57cc6f976a76"},"source":["print('Recall that our training features array train_X has')\n","print(f'm = {train_X.shape[0]} rows and n = {train_X.shape[1]} columns')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Recall that our training features array train_X has\n","m = 66 rows and n = 73 columns\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BSvkrQ6EdnN8"},"source":["`train_X` is thus wider than it is tall, which suggests that the regularized normal equations might perform better in generating label predictions. In this case, we modify the first of the normal equations given above to \n","\n","$$\\boldsymbol{\\theta} = (X^T \\cdot X + \\lambda m I)^{-1} \\cdot X^T \\cdot Y$$.\n","\n","Here, $\\lambda$ is the regularization parameter and $m$ is the number of rows in $X$."]},{"cell_type":"markdown","metadata":{"id":"7Mgd1zocdnN9"},"source":["### Repeat the previous parts of Task IV, but this time incorporate regularization"]},{"cell_type":"code","metadata":{"id":"nFY5YO-IZ2e7"},"source":["def regularized_normal_equations_solution(X, Y, regularization_param):\n","  X_with_ones = np.hstack([np.ones([train_X.shape[0], 1]), train_X])\n","  ## YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bIrbJkOydnN9"},"source":["test_these_lambdas = np.linspace(0.01, 10, 10)\n","\n","for i in test_these_lambdas:\n","  theta_temp = regularized_normal_equations_solution(train_X, train_Y, i)\n","  pred_Y_from_N_reg = np.matmul(np.hstack([np.ones([test_X.shape[0], 1]), test_X]), theta_temp)\n","  pred_Y_from_N_reg[np.where(pred_Y_from_N_reg<0)]=0\n","  print('For regularization parameter', i)\n","  print(\"RMSE, R2 =\", MSE(test_Y, pred_Y_from_N_reg), r2_score(test_Y, pred_Y_from_N_reg))\n","  print('-----')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5uD0DVidnN_"},"source":["# Task V: Non-linear Regression Models (GLM, DT) "]},{"cell_type":"markdown","metadata":{"id":"Yk0Be7SQdnN_"},"source":["### Generalized Linear Models\n","\n","`sm` (our alias for `statsmodels.api`) contains a `GLM` class. Use it to instantiate a model. The relevant parameters are training labels, training features, and `ffamily`, i.e. the family of distributions to which we assume our prediction errors belong. Some potentially good choices for `ffamily` include Gaussian, Gamma, and Logit."]},{"cell_type":"code","metadata":{"id":"cxUdD_LhdnN_"},"source":["# GLM \n","import statsmodels.api as sm\n","# Instantiate the GLM\n","train_X_glm = sm.add_constant(train_X)\n","glm_gamma = sm.GLM(train_Y, train_X_glm, family=sm.families.Gaussian())\n","# Train the GLM\n","glm_results = glm_gamma.fit()\n","print(glm_results.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7IAchWHodnOB"},"source":["### Generate predictions from the test data"]},{"cell_type":"code","metadata":{"id":"XLTsITX7dnOC"},"source":["# generate predictions, called pred_Y_from_GLM. (don't forget to add a constant column to test_X)\n","## YOUR CODE HERE\n","# Set any negative predictions to 0\n","pred_Y_from_GLM[pred_Y_from_GLM<0]=0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TgXIT-JpdnOE"},"source":["# Plot predictions compared to test_Y, and also print MSE and R^2 score\n","## YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vld0rNUcdnOG"},"source":["### Random Forest Regression\n","\n","Use the `RandomForestRegressor` from `sklearn.ensemble` to generate predictions. The relevant parameters are the `max_depth` of the trees and the `random_state`, to ensure reproducibility."]},{"cell_type":"code","metadata":{"id":"EbxRUm3UdnOG"},"source":["from sklearn.ensemble import RandomForestRegressor\n","# Instantiate the random forest regression model\n","regr = RandomForestRegressor(max_depth=5, random_state=0)\n","# Train the model\n","## YOUR CODE HERE\n","# Generate predictions from the test data\n","## YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YSZX9t1MdnOK"},"source":["# Plot predictions compared to test_Y, and also print MSE and R^2 score\n","## YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bxx1JrikdnON"},"source":["\n","## Populate the table below with the results of your experiments above. Which models performed best?"]},{"cell_type":"markdown","metadata":{"id":"44MPnft4dnON"},"source":["## Results\n","\n","|Method      |RMSE             |R2               |\n","|------------|-----------------|-----------------|\n","| Gradient Descent | xx | xx|\n","| Normal Equations | xx | xx |\n","| Regularized Normal Equations | xx | xx |\n","| Generalized Linear Model (GLM) | xx | xx |\n","| Random Forests | xx | xx |\n"]}]}