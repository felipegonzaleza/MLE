{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"week10_V1.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"tyN5gj274OgN"},"source":["# Today you are an MLE in the Personalization Department in Macy's cosmetics!\n","Your goal is to predict outcomes of online browsing sessions, namely predicting if the next sequence of events/session will result in a purchase or not. \n","\n","Models used in this assignment are similar to https://github.com/guillaume-chevalier/seq2seq-signal-prediction/blob/master/seq2seq.ipynb\n","\n"]},{"cell_type":"markdown","metadata":{"id":"G72VpQoe9qyi"},"source":["# Task 0: Getting familiar with the Data"]},{"cell_type":"markdown","metadata":{"id":"jPFS7fBeoeP-"},"source":["...if you're in Colab..."]},{"cell_type":"code","metadata":{"id":"qLLIlcrdvmw1"},"source":["#Mount the RAW session level data: shopping.pkl\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qbh2P_qs9i1_"},"source":["# Import all libraries\n","import numpy as np\n","import pandas as pd\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Bidirectional"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S5UdJL8sP6fz"},"source":["#Read and look at the RAW data\n","data = pd.read_pickle('/content/drive/MyDrive/Datasets/week_10/Sequence_Models/shopping.pkl') # This is where I stored my data. Where'd you put yours?\n","print('Shape of data=', data.shape)\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xGvjhdUnu2SB"},"source":["So, for each unique user_session ID, the data fields that are collected are the time the event occured, the type of event (view, cart, remove, purchase), the product ID, category ID, brand, price, user ID, year, month, weekday, and hour. Notice that we have 1.5M data samples."]},{"cell_type":"markdown","metadata":{"id":"qW9BT7eS-oH7"},"source":["# Task 1: Set up and train a simple RNN on this time series data\n","\n","There's a lot of data we could use here, but we're going to start with something pretty simple. Think of a user session as being made up of a series of events (e.g. ['view', (add to)'cart', 'view', (add to)'cart', 'purchase']). We want a model that can take in a series of non-purchase events, and predict whether a purchase is going to occur. In the example above, ['view', (add to)'cart', 'view', (add to)'cart'] is a **sequence** which culminates in a purchase. We'll create a representation of sequences of events, and train a model to predict whether or not a purchase is going to occur. \n","\n","This work is the modification from the paper: https://arxiv.org/ftp/arxiv/papers/2010/2010.02503.pdf"]},{"cell_type":"markdown","metadata":{"id":"1nW-ZTezxAEw"},"source":["## Step 1: Create sequence data for each session"]},{"cell_type":"code","metadata":{"id":"z7fl9XZMP6f0"},"source":["# Convert the event types to numeric values\n","events = {'purchase':1,'cart': 2,'view': 3, 'remove_from_cart':4}\n","data['event'] = data.event_type.map(events)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_XPpZb68P6f0"},"source":["# Sort the events by 'event_time'\n","data = data.sort_values('event_time')\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DmU65kWatAWs"},"source":["^ The `event` column we added is just a numerical representation of the of the event type (purchase, cart, view, remove_from_cart)"]},{"cell_type":"markdown","metadata":{"id":"kkFzILKzkRrt"},"source":["Next, process the data into a new dataframe `sequence`, to have three columns:\n","1. There will still be a `user_session` column, but now only one row for each user session\n","2. The `event` column will now be a list of the events that occured in that session, in the order they occured. \n","3. A `purchase` column indicates whether the events for that session included a purchase. The number 1 will indicate a purchase, 0 will indicate no purchase."]},{"cell_type":"code","metadata":{"id":"YEOeD6osP6f1"},"source":["sequence = data.groupby('user_session')['event'].apply(list)\n","sequence = sequence.reset_index()\n","sequence['purchase'] = sequence['event'].apply(lambda x: 1 if 1 in x else 0)\n","sequence = sequence[sequence['event'].map(len)> 1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FFTXy5tnttIC"},"source":["sequence.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fOgMv-STw6m0"},"source":["...one problem we have is that the event entries still have '1's for the purchase events, let's get rid of those because their presence is already indicated by the purchase column... "]},{"cell_type":"code","metadata":{"scrolled":true,"id":"fl26fdnvP6f4"},"source":["#The sequence data should not contain the \"purchase field\" so it is filtered out\n","sequence['event']= sequence.event.apply(lambda row: list(filter(lambda a: a != 1, row)))\n","print('Total number of records=', sequence.shape[0])\n","sequence.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-8xsenMYyKHI"},"source":["temp_one_hot = np.array(pd.get_dummies(sequence['purchase'],prefix='Purchase'))\n","fraction_with_purchase=np.sum(temp_one_hot[:,1])/len(temp_one_hot)\n","print('Fraction of sessions ending in a purchase:', fraction_with_purchase)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pU6mRjRiEGHZ"},"source":["What does an average sequence look like? Can some sequences be especially long and others be very short?"]},{"cell_type":"code","metadata":{"id":"K2Mf3A5yP6f4"},"source":["#Find the length of events per user-session\n","length = sequence['event'].map(len).to_list()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MJrKuMCDP6f5"},"source":["import seaborn as sns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"agn_AM6dP6f5"},"source":["sns.distplot(length)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"alxwobvPP6f6"},"source":["### So we see that most sequences are about 100 or shorter. \n","One difficult task in time series modeling is to find the optimal sequence size for highest prediction accuracy. We won't attemp that today, but let's at least split the data up into short and long sequences, to see if there's a difference in models trained on these different sequence lengths. \n","### We'll focus on sequences that have lengths less than or equal to 10."]},{"cell_type":"code","metadata":{"id":"dLZ-S5lxP6f6"},"source":["# select all sequences that are upto 10 events long. Discard remaining sequences.\n","short_sequence_10 = sequence[sequence['event'].map(len) <= 10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fzy_4xh3P6f7"},"source":["#Lets see how many records come up\n","short_sequence_10"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uDScrStaFqnm"},"source":["Let's do a little more preprocessing to arrange this data in a form amenable to training a TensorFlow neural network..."]},{"cell_type":"code","metadata":{"id":"K6qRStdGP6f7"},"source":["event_sequence = short_sequence_10['event'].to_list()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8p65PEhX7Iwz"},"source":["event_sequence[0:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MKLHK-SdP6f8"},"source":["# Pad all sequences with zeros so all inputs have same consistent size of 10\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","event = pad_sequences(event_sequence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gScNr6Pi7Ph8"},"source":["event[0:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"ZuOIbSAcP6f8"},"source":["# One Hot Encoding the Purchase label\n","y = np.array(pd.get_dummies(short_sequence_10['purchase'],prefix='Purchase'))\n","z=np.sum(y[:,1])/len(y)\n","print('Fraction of purchase sessions for the length-10 sequence data:',z)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BbwdeT39h6l5"},"source":["#Define a function to generate 70/30 data split followed by data resizing\n","def prepare_train_test_data(data,y):\n","  #input is data[nxd] and Y[nx2], outputs 70/30 split formatted for the sequence models\n","  X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.3)\n","  #Resizing is necessary since input to Tensorflow sequence models is (1,d)\n","  X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n","  X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n","  return (X_train,X_test,y_train,y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rjK0Z2WQP6f9"},"source":["X_train, X_test, y_train, y_test=prepare_train_test_data(np.array(event),y)\n","print(X_train.shape, y_train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"krjPyvUzP6f_"},"source":["These helper functions below are just used to display model accuracy during training, and also evaluate the performance of a trained model:"]},{"cell_type":"code","metadata":{"id":"wlp3503JIL83"},"source":["import matplotlib.pyplot as plt\n","\n","# demonstration of calculating metrics for a neural network model using sklearn\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","\n","\n","def plot_history(history):\n","  # This function will plot the model fit process\n","  print(history.history.keys())\n","  # summarize history for accuracy\n","  plt.plot(history.history['acc'])\n","  plt.plot(history.history['val_acc'])\n","  plt.title('model accuracy')\n","  plt.ylabel('acc')\n","  plt.xlabel('epoch')\n","  plt.legend(['train', 'test'], loc='upper left')\n","  plt.show()\n","  # summarize history for loss\n","  plt.plot(history.history['loss'])\n","  plt.plot(history.history['val_loss'])\n","  plt.title('model loss')\n","  plt.ylabel('loss')\n","  plt.xlabel('epoch')\n","  plt.legend(['train', 'test'], loc='upper left')\n","  plt.show()\n","\n","\n","def evaluate_on_test(X_test, y_test, training_model):\n","  #This function will evaluate the fit model on test data\n","  model_output=training_model.predict(X_test)\n","  g_preds = np.argmax(model_output,axis=1)\n","  gaccuracy = accuracy_score(y_test[:,1], g_preds)\n","  print('Accuracy: %f' % gaccuracy)\n","  # precision tp / (tp + fp)\n","  gprecision = precision_score(y_test[:,1], g_preds)\n","  print('Precision: %f' % gprecision)\n","  # recall: tp / (tp + fn)\n","  grecall = recall_score(y_test[:,1], g_preds)\n","  print('Recall: %f' % grecall)\n","  # f1: 2 tp / (2 tp + fp + fn)\n","  gf1 = f1_score(y_test[:,1], g_preds)\n","  print('F1 score: %f' % gf1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ORGIGQSI3Ei"},"source":["We'll start with training a [`SimpleRNN`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN) and then move on to fancier things."]},{"cell_type":"code","metadata":{"id":"ytK1F5UMI6EC"},"source":["from tensorflow.keras.layers import GRU, Embedding, SimpleRNN, Activation\n","import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p8fOqyJPJDc5"},"source":["#This is a simple RNN model\n","def simple_RNN_model(neurons=40, op=10):\n","    model = Sequential()\n","    model.add(SimpleRNN(neurons, return_sequences = True, input_shape = (1,op))) ##neurons: units (layer output shape), op: input vector size = sequence length\n","    model.add(SimpleRNN(2*neurons))\n","    model.add(Dense(2, activation='softmax'))\n","    model.compile(\n","      optimizer=tf.optimizers.Adam(learning_rate=0.0003),\n","      loss='binary_crossentropy',\n","      metrics=['acc'])\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1cuwPUcLJvh6"},"source":["#Visualize the Model\n","tf.keras.backend.clear_session()\n","RNN_model = simple_RNN_model(neurons=40, op=10)\n","RNN_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IhdK1Xd6JJCw"},"source":["#Fit the model using 80/20 validation split at runtime\n","r_history = RNN_model.fit(X_train, y_train,\n","                    epochs=20,\n","                    batch_size=1000,\n","                    validation_split=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zVJTgqvHJJFB"},"source":["plot_history(r_history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZRTCPXThJ38p"},"source":["evaluate_on_test(X_test,y_test,RNN_model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4QNQqrnx-Xro"},"source":["Okay, so that was pretty straightforward--we preprocessed the data to have numerical sequences of length 10 (corresponding to different online shoping events), and then used a model with two SimpleRNN layers from TensorFlow to train on this data. The recall here is fairly low, ostensibly because of the imbalance of purchase and non-purchase events, but this was a good first start."]},{"cell_type":"markdown","metadata":{"id":"-IULjM4vLIUx"},"source":["# Task 2: Train GRU-based and LSTM-based models on the length-10 sequence data.\n"]},{"cell_type":"markdown","metadata":{"id":"0I82RU-x_qHm"},"source":["Define a network based on GRU layers called `GRU_model`. Just copy and paste the architecture of `simple_RNN_model` and exchange the `SimpleRNN` layers for `GRU` layers (these have already been imported above)."]},{"cell_type":"code","metadata":{"id":"s-zDi5ykLq2M"},"source":["def GRU_model(neurons=40, op=10):\n","  model = Sequential()\n","  ############### START CODE HERE ################\n","  ####### END CODE HERE #######\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DEy3TOkMLy8T"},"source":["#Visualize the Model\n","tf.keras.backend.clear_session()\n","G_model = GRU_model()\n","G_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fJCyU-4XAz81"},"source":["^ Note the increase in the number of parameters."]},{"cell_type":"code","metadata":{"id":"pcYsLzXWMIiB"},"source":["#Train the G_model (40 epochs, 1000 samples per batch, validation split=0.2)\n","g_history = G_model.fit(X_train, y_train,\n","                        epochs=20,\n","                        batch_size=1000,\n","                        validation_split=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0GdxIFEcMS94"},"source":["#Use plot_history function to plot the model curves for loss and accuracy\n","plot_history(g_history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f4vFqrzPMWmY"},"source":["# Use evaluate_on_test function to note accuracy, precision, recall and F1 score on test data\n","evaluate_on_test(X_test,y_test, G_model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qqQ4Y05we6Xn"},"source":["These may get a very similar performance as the simple RNN, but you can check that it does produce slightly different outputs. GRU recurrent neural networks are just a more complicated type of network and often perform better on sequence prediction tasks. "]},{"cell_type":"markdown","metadata":{"id":"tfZHhIZW9zXI"},"source":["Do the same as you did above for GRU, but this time for a model based on [`LSTM`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) layers. Try wrapping the first LSTM layer with [`Bidirectional`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional).\n","\n"]},{"cell_type":"code","metadata":{"id":"62VFEslyqEWX"},"source":["#Define an LSTM model function. Use the LSTM layer as shown below.\n","#Notice the change in number of parameters\n","def LSTM_model(neurons=40, op=10):\n","  model = Sequential()\n","  #### YOUR CODE HERE #####\n","  #### END CODE HERE #####\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1HZqWlbtqegR"},"source":["tf.keras.backend.clear_session()\n","l_model = LSTM_model(neurons=40, op=10)\n","l_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"duvgEdL-rur0"},"source":["#Train the training_model (20 epochs, 1000 samples per batch, validation split=0.2)\n","l_history = l_model.fit(X_train, y_train,\n","                    epochs=20,\n","                    batch_size=1000,#atleast 1000 records per epoch\n","                    validation_split=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5MF7_nuj2ZQO"},"source":["#Use plot_history function to plot the model curves for loss and accuracy\n","plot_history(l_history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KAGWtCN-OKrG"},"source":["# Use evaluate_on_test function to note accuracy, precision, recall and F1 score on test data\n","evaluate_on_test(X_test,y_test,l_model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ys0VKUs3hBQf"},"source":["...LSTMs can sometimes take longer to fit because they have more free parameters."]},{"cell_type":"markdown","metadata":{"id":"2_5HNGfkP7I6"},"source":["# Task 3: Let's try running the GRU model on a different dataset with more informative features."]},{"cell_type":"code","metadata":{"id":"Ku0_TrjDuDhu"},"source":["#Next lets look at some other session \"features\"\n","feat = pd.read_pickle('/content/drive/MyDrive/Datasets/week_10/Sequence_Models/Session_features.pkl')\n","print('Shape of data=', feat.shape)\n","feat.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"owWV1PK9qlsy"},"source":["This is just a different dataset that's focused on whole sessions, for which we have more different types of features (36 of them)"]},{"cell_type":"markdown","metadata":{"id":"cOxWBmtGlqca"},"source":["Suppose we were to treat each row of features as a \"sequence.\" This is not the most common setup for RNNs, (since each element in the sequence (each column) represents different types of quantities) but it'll work in our case. We'll take the first 35 columns as the elements of our sequence and the 36th column, the purchase column, as the target:"]},{"cell_type":"code","metadata":{"id":"QQHFIkXGI6zs"},"source":["Xf=feat.iloc[:,0:35]\n","Yf=feat.iloc[:,35]\n","yf = np.array(pd.get_dummies(Yf, prefix='Purchase'))\n","Xf_train, Xf_test, yf_train, yf_test=prepare_train_test_data(np.array(Xf), np.array(yf)) # Function call to 'prepare_train_test_data' to create 70/30 split data\n","print(Xf_train.shape, yf_train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GuxjWsu8QPhi"},"source":["#Initialize ANY model (RNN or GRU or LSTM)\n","def GRU_model(neurons=40, op=10):\n","  model = Sequential()\n","  model.add(GRU(neurons, return_sequences = True, input_shape = (1,op)))\n","  model.add(GRU(2*neurons))\n","  model.add(Dense(2, activation='softmax'))\n","  model.compile(\n","    optimizer=tf.optimizers.Adam(learning_rate=0.0003),\n","    loss='binary_crossentropy',\n","    metrics=['acc'])\n","  return model\n","\n","#Visualize the Model (Notice the increase in parameters)\n","tf.keras.backend.clear_session()\n","gru_model = GRU_model(neurons=40, op=Xf_train.shape[2])\n","gru_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zwukFzm5yYcj"},"source":["#Fit your model on Training data (20 epochs, 1000 samples per batch, validation_split=0.2)\n","gru_history = gru_model.fit(Xf_train, yf_train,\n","                    epochs=20,\n","                    batch_size=1000,#atleast 1000 records per epoch\n","                    validation_split=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ILXRxuM_ymWm"},"source":["#Use plot_history function to plot the model curves for loss and accuracy\n","plot_history(gru_history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3BhRFSlcyqhT"},"source":["# Use evaluate_on_test function to note accuracy, precision, recall and F1 score on test data\n","evaluate_on_test(Xf_test,yf_test,gru_model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ocx8Ydrey0-W"},"source":["#Comment on what you would suggest to your manager?\n","\n","\n","*  Is feature-level data necessary? What metrics suggest that?\n","*  What you you suggest to improve the quality of the model trained on event-sequence data (the first dataset?) \n","\n"]},{"cell_type":"markdown","metadata":{"id":"x7zzlkM1oX0k"},"source":[""]}]}